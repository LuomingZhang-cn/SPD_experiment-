# -*- coding: utf-8 -*-
"""SPD_Simulation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WxL8J7ENHBX2etAcxQPJcQh-Mr73PoeE

## 1. Environment Setup and Dependency Installation
"""

!pip install langchain-deepseek langchain langchain-core langchain-community
!pip install deepseek-api
!pip install tqdm

import random
import numpy as np
import pandas as pd
import logging
import matplotlib.pyplot as plt
import seaborn as sns
from langchain_deepseek import ChatDeepSeek
from langchain.schema import HumanMessage
from google.colab import userdata
import time

"""## 2. API and Model Configration

### 2.1 配置API
"""

# First, install the OpenAI library
from openai import OpenAI
from google.colab import userdata

# Get the DeepSeek API key (store and retrieve API key via Google Colab's userdata)
api_key = userdata.get('DeepSeek_API_KEY')

# Ensure api_key is correctly passed
if api_key is None:
    raise ValueError("DeepSeek API key is missing")

# Initialize the DeepSeek client, ensuring the correct base_url is used
client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

"""### 2.2 Initialise the DeepSeek client"""

from langchain_deepseek import ChatDeepSeek
from langchain_core.prompts import ChatPromptTemplate

import os
os.environ["DEEPSEEK_API_KEY"] = api_key

# Initialize the model
model = ChatDeepSeek(
    model_name="deepseek-chat",
    temperature=0.4,
    max_tokens=None,
    timeout=None,
    api_key=api_key
)

# Create the prompt template
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a strategic agent in a spatial decision-making game."),
    ("human", "{input}")
])

# Construct the base chain
chain = prompt | model

"""## 3 LLM Agent Class Definition

### 3.1 SPD Agent Construction
"""

from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage

import random
import time

class SPDAgent:
    def __init__(self, agent_id, age, gender, traits, personality_desc):
        self.id = agent_id
        self.age = age
        self.gender = gender
        self.traits = traits
        self.personality_desc = personality_desc
        self.action = random.choice(["blue", "yellow"])
        self.prev_action = None
        self.payoff = 0
        self.history = []  # Stores (round, action, payoff) tuples

    def update(self, new_action, payoff):
        """Updates agent's state"""
        self.prev_action = self.action  # Save previous round's action
        self.action = new_action        # Update current action
        self.payoff = payoff            # Update current payoff
        # Record history: (round, action, payoff)
        self.history.append((len(self.history)+1, new_action, payoff))

    def generate_prompt(self, neighbors, treatment):
        """Generates the decision prompt"""
        neighbor_info = []
        for n in neighbors:
            info = f"- Neighbor {n['id']}: chose {n['action']}"
            if treatment == "TWI":
                info += f", earned {n['payoff']} points"
            neighbor_info.append(info)
        # gane rules
        game_rules = (
            "Respond with ONLY 'blue' (Cooperate) or 'yellow' (Defect).\n"
            "Payoff rules:\n"
            "- If you choose 'blue' and neighbor chooses 'blue': both earn 5 points\n"
            "- If you choose 'blue' but neighbor chooses 'yellow': you earn 0, neighbor earns 6\n"
            "- If you choose 'yellow' but neighbor chooses 'blue': you earn 6, neighbor earns 0\n"
            "- If both choose 'yellow': both earn 1 point\n"
        )

        # prompt for agents
        prompt = (
            f"You are in a spatial Prisoner's Dilemma game. Your goal is to maximize payoff.\n"
            f"{game_rules}\n"
            f"Your personality: {self.personality_desc}\n"
            f"Last round: You chose {self.prev_action or self.action}, earned {self.payoff} points\n"
            f"Neighbors' last actions{' and earnings' if treatment=='TWI' else ''}:\n" +
            "\n".join(neighbor_info) + "\n\n" +
            "Decide your action. Respond ONLY with 'blue' or 'yellow'."
        )
        return prompt

    def decide_action(self, neighbors, treatment):
        """Uses DeepSeek for decision-making"""
        prompt = self.generate_prompt(neighbors, treatment)
        max_retries = 3
        for attempt in range(max_retries):
          try:
            response = chain.invoke({"input": prompt}).content.strip().lower()
            if "blue" in response:
                return "blue"
            elif "yellow" in response:
                return "yellow"
            else:  # If response is unclear
                return random.choice(["blue", "yellow"])
          except Exception as e:
              print(f"Agent {self.id} decision error (attempt {attempt+1}): {e}")
              time.sleep(2)  # Wait before retrying
          # Fallback after all retries fail
          return self.prev_action or random.choice(["blue", "yellow"])

"""### 3.2 Creating a proxy pool"""

import random

def create_agent_pool(num_agents=144):
    """
    Creates a heterogeneous pool of agents.
    Args:
        num_agents: The number of agents in the pool (default 144 agents).
    Returns:
        List[SPDAgent]: A list of agent objects.
    """
    big_five = ["openness", "conscientiousness", "extraversion", "agreeableness", "neuroticism"]
    age_bins = {"19-22": (19, 22), "23-26": (23, 26), "27-29": (27, 29)}
    personality_descriptors = {
        "openness": {
            "low": "less open to new experiences",
            "medium": "moderately open to new experiences",
            "high": "highly open to new experiences"
        },
        "conscientiousness": {
            "low": "less conscientious and organized",
            "medium": "moderately conscientious and organized",
            "high": "highly conscientious and organized"
        },
        "extraversion": {
            "low": "more introverted",
            "medium": "moderately extraverted",
            "high": "highly extraverted"
        },
        "agreeableness": {
            "low": "less agreeable and cooperative",
            "medium": "moderately agreeable and cooperative",
            "high": "highly agreeable and cooperative"
        },
        "neuroticism": {
            "low": "emotionally stable and calm",
            "medium": "moderately prone to negative emotions",
            "high": "more prone to negative emotions"
        }
    }

    agents = []
    for i in range(num_agents):
        # Age group
        age_group = random.choice(list(age_bins.keys()))
        age = random.randint(*age_bins[age_group])

        # Gender ratio 54% male
        gender = "male" if random.random() < 0.54 else "female"

        # Randomly generate Big Five personality traits
        traits = {dim: random.choice(["low", "medium", "high"]) for dim in big_five}

        # Generate personality description
        desc_parts = [f"{personality_descriptors[dim][traits[dim]]}" for dim in big_five]
        desc = f"You are a {gender} aged {age}. Personality: {'; '.join(desc_parts)}."

        # Create SPDAgent object
        agent = SPDAgent(i, age, gender, traits, desc)
        agents.append(agent)

    return agents

"""## 4. Initialise proxy grid and neighbour construction"""

import numpy as np

def init_grid(agents):
    """
    Initializes a 4x4 grid and assigns neighbor relationships.
    Args:
        agents: A list of 16 agents.
    Returns:
        dict: The grid data structure.
    """
    grid_size = 4  # Set grid size to 4x4
    grid = {}

    # Create grid position mapping
    for idx, agent in enumerate(agents):
        row = idx // grid_size
        col = idx % grid_size
        grid[(row, col)] = agent  # Place the agent at the corresponding grid position

    # Function for periodic boundary handling
    def get_neighbor_pos(row, col):
        """Gets the 4 neighbor positions for a given location."""
        return [
            ((row-1) % grid_size, col),  # Up
            ((row+1) % grid_size, col),  # Down
            (row, (col-1) % grid_size),  # Left
            (row, (col+1) % grid_size)   # Right
        ]

    # Assign neighbors to each agent
    for (row, col), agent in grid.items():
        neighbor_positions = get_neighbor_pos(row, col)
        agent.neighbors = [grid[pos] for pos in neighbor_positions]  # List of neighbors

    return list(grid.values())  # Return the list of agent objects

"""## 5. Experiment execution and data recording"""

import random
import numpy as np # This import might be needed if calculate_payoff used np.array in an earlier version

def calculate_payoff(agent_action, neighbor_action):
    """
    Calculates the prisoner's dilemma payoff.
    Args:
        agent_action: Agent's action ('blue'/'yellow')
        neighbor_action: Neighbor's action ('blue'/'yellow')
    Returns:
        tuple: (Agent's payoff, Neighbor's payoff)
    """
    # Payoff matrix (agent_payoff, neighbor_payoff)
    if agent_action == "blue" and neighbor_action == "blue":
        return (5, 5)
    elif agent_action == "blue" and neighbor_action == "yellow":
        return (0, 6)
    elif agent_action == "yellow" and neighbor_action == "blue":
        return (6, 0)
    else:  # yellow vs yellow
        return (1, 1)

def run_one_round(agents, treatment, round_num, session_id):
    # First save all agents' previous payoffs (from last round)
    prev_payoffs = {agent.id: agent.payoff for agent in agents}  # Make sure this line is inside the function body and properly indented

    # Phase 1: All agents make decisions
    for agent in agents:
        # Collect neighbor information (only direct neighbors)
        neighbor_info = []
        for n in agent.neighbors:  # These are already the 4 von Neumann neighbors
            neighbor_info.append({
                "id": n.id,
                "action": n.action,
                "payoff": n.payoff  # Using neighbor's payoff from previous round
            })
        agent._new_action = agent.decide_action(neighbor_info, treatment)

    # Phase 2: Calculate payoffs and record data
    round_data = []
    for agent in agents:
        total_payoff = 0
        neighbor_prev_payoffs = [prev_payoffs[n.id] for n in agent.neighbors]  # Get from dictionary

        # Calculate current round payoff
        for neighbor in agent.neighbors:
            _, neighbor_payoff = calculate_payoff(agent._new_action, neighbor._new_action)
            total_payoff += neighbor_payoff

        # Calculate diff (only for TWI treatment)
        diff = None
        if treatment == "TWI" and neighbor_prev_payoffs:
            current_agent_prev = prev_payoffs[agent.id]
            max_neighbor_prev = max(neighbor_prev_payoffs)
            diff = max_neighbor_prev - current_agent_prev

        # Update agent state
        prev_action = agent.action
        agent.update(agent._new_action, total_payoff)

        # Record data
        data_point = {
            "session_id": session_id,
            "agent_id": agent.id,
            "round": round_num,
            "treatment": treatment,
            "action": 1 if agent.action == "blue" else 0,
            "prev_action": 1 if prev_action == "blue" else 0,
            "switch": 1 if agent.action != prev_action else 0,
            "payoff": total_payoff,
            "N_coop": sum(1 for n in agent.neighbors if n.action == "blue"),
            "diff": diff,  # Could be None (TWO) or a value (TWI)
            "age": agent.age,
            "gender": agent.gender,
            "traits": str(agent.traits)
        }
        round_data.append(data_point)

    return round_data

"""## 6. Conduct the main experiment"""

import time
import random
from tqdm import tqdm
import pandas as pd
from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive')

def run_experiment(agent_pool, treatment_type="TWI", num_sessions=9, rounds_per_session=50):
    """
    Experiment function ensuring each agent participates in only one complete session.
    Args:
        agent_pool: List of all available agents
        treatment_type: Experiment type ("TWI" or "TWO")
        num_sessions: Total number of sessions (default 9)
        rounds_per_session: Rounds per session (default 50)
    Returns:
        pd.DataFrame: DataFrame containing all experiment data
    """
    # Check if agent count matches requirements
    total_agents_needed = num_sessions * 16
    if len(agent_pool) != total_agents_needed:
        raise ValueError(f"Exactly {total_agents_needed} agents required, but got {len(agent_pool)}")

    # Create save directory
    save_dir = '/content/drive/MyDrive/SPD_experiment'
    os.makedirs(save_dir, exist_ok=True)

    # Shuffle agent order randomly
    random.shuffle(agent_pool)
    remaining_agents = agent_pool.copy()  # Agents not yet assigned
    all_data = []

    with tqdm(total=num_sessions*rounds_per_session, desc="Experiment Progress") as pbar:
        for session_index in range(num_sessions):
            # Use the passed treatment_type directly (no dynamic assignment)
            treatment = treatment_type  # Fixed to the passed type (TWI or TWO)
            session_agents = remaining_agents[:16]
            remaining_agents = remaining_agents[16:]
            session_id = session_index + 1

            # Display assignment information
            agent_ids = sorted([a.id for a in session_agents])
            pbar.write(f"Session {session_id}({treatment}) Assigned agent IDs: {agent_ids[0]}-{agent_ids[-1]}")

            # Initialize grid
            agents_list = init_grid(session_agents)

            # Run all rounds for current session
            for round_num in range(1, rounds_per_session + 1):
                round_data = run_one_round(agents_list, treatment, round_num, session_id)
                all_data.extend(round_data)
                pbar.update(1)
                pbar.set_postfix({
                    "Session": f"{session_id}/{num_sessions}",
                    "Remaining Agents": len(remaining_agents),
                    "Round": f"{round_num}/{rounds_per_session}"
                })

    # Save data
    df = pd.DataFrame(all_data)
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    final_path = f'{save_dir}/SPD_results_{timestamp}.csv'
    df.to_csv(final_path, index=False)

    if remaining_agents:
        raise RuntimeError(f"Still {len(remaining_agents)} agents unassigned!")

    print(f"\nExperiment completed! Total agents used: {num_sessions*16}")
    print(f"Treatment type: {treatment_type}")
    print(f"Total data points: {len(all_data)} records")
    print(f"Results saved to: {final_path}")
    return df

# Create 144 agents
agent_pool = create_agent_pool(num_agents=144)

# Randomly assign 80 agents to TWI treatment
twi_agents = random.sample(agent_pool, 80)
print(f"Selected {len(twi_agents)} agents for TWI: {[a.id for a in twi_agents[:5]]}...")

# Run TWI experiment
df_twi = run_experiment(twi_agents, treatment_type="TWI", num_sessions=5, rounds_per_session=50)
df_twi['treatment'] = 'TWI'  # Add treatment column
df_twi.to_csv("TWI_data.csv", index=False)

# Remaining agents go to TWO treatment
two_agents = [a for a in agent_pool if a not in twi_agents]
print(f"Remaining {len(two_agents)} agents for TWO: {[a.id for a in two_agents[:5]]}...")

# Run TWO experiment
df_two = run_experiment(two_agents, treatment_type="TWO", num_sessions=4, rounds_per_session=50)
df_two['treatment'] = 'TWO'
df_two.to_csv("TWO_data.csv", index=False)

# Combine results
full_df = pd.concat([df_twi, df_two])
full_df.to_csv("full_experiment_data.csv", index=False)
print(f"Total data: {len(full_df)} rows")